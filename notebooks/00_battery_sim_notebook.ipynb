{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808992f",
   "metadata": {},
   "source": [
    "# Energy Storage System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bbc13",
   "metadata": {},
   "source": [
    "In this notebook, we will demontrate how to train an RL agent for Energy Storage System (ESS) arbitrage. \n",
    "\n",
    "The simulated energy environment is created based on the paper [Arbitrage of Energy Storage in Electricity Markets with Deep Reinforcement Learning](https://arxiv.org/abs/1904.12232)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c9a98",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "### Python Package\n",
    "Ensure that you python vitural environment have installed the required python packages in `requirements.txt`\n",
    "\n",
    "### Dataset\n",
    "Download dataset from [here](https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data) and placed into `data` folder as follows:\n",
    "\n",
    "```\n",
    "|-- data\n",
    "|   `-- PRICE_AND_DEMAND_202106_NSW1.csv\n",
    "```\n",
    "\n",
    "You can choose to use one month of data or manually concatenate multiple months depending on your use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1819ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to download the sample data to a local file called data/sample-data.csv\n",
    "!mkdir data/\n",
    "!curl https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_202103_NSW1.csv > data/sample-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a37da5",
   "metadata": {},
   "source": [
    "## Battery Environment Simulator\n",
    "\n",
    "We start by building a energy storage system environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962792b",
   "metadata": {},
   "source": [
    "You need to set `env_config={\"LOCAL\": True}` to use data from local src folder instead of S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import List\n",
    "from battery_env_sm import SimpleBattery\n",
    "from report import Report, ReportIO, plot_reward, plot_analysis\n",
    "from nbutils import evaluate_episode\n",
    "\n",
    "import seaborn as sns    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "env_config = {\"MAX_STEPS_PER_EPISODE\": 168, \"LOCAL\": True, \"FILEPATH\": \"data/sample-data.csv\"}\n",
    "\n",
    "EPISODE = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51e750",
   "metadata": {},
   "source": [
    "## (0) Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from battery import SimpleBattery\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    Random agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: List):\n",
    "        self.actions = actions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = np.random.choice([0, 1, 2])\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1)\n",
    "    env = SimpleBattery(env_config)\n",
    "    agent = SimpleAgent([SimpleBattery.CHARGE, SimpleBattery.DISCHARGE, SimpleBattery.HOLD])\n",
    "    rewards_list = []\n",
    "    history_list: List = []\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            history_list.append([i] + [total_rewards] + [action] + state)\n",
    "            state = next_state\n",
    "\n",
    "        # print(f\"Episode {i+1} ({env.counter}):{total_rewards}\")\n",
    "        rewards_list.append(total_rewards)\n",
    "\n",
    "    average_reward = sum(rewards_list) / len(rewards_list)\n",
    "    print(f\"Average reward: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reward(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f4310",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429425cf",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The agent action is totally random, regardless of price and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bf86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "df_eval = evaluate_episode(agent, env_config)\n",
    "fig = plot_analysis(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29b733",
   "metadata": {},
   "source": [
    "## (1) Market price vs cost agent\n",
    "\n",
    "- SELL: when market price is higher than cost\n",
    "- BUY: when market price is lower than cost\n",
    "- HOLD: others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6353e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    What should be the initial initial energy costs?\n",
    "\n",
    "    Buy: electric price < electric cost\n",
    "    Sell: electric price > electric cost\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: List):\n",
    "        self.actions = actions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        electric_price = state[2]\n",
    "        electric_cost = state[1]\n",
    "\n",
    "        if electric_price > electric_cost:\n",
    "            action = SimpleBattery.DISCHARGE\n",
    "        elif electric_price < electric_cost:\n",
    "            action = SimpleBattery.CHARGE\n",
    "        else:\n",
    "            action = SimpleBattery.HOLD\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1)\n",
    "    env = SimpleBattery(env_config)\n",
    "    agent = SimpleAgent([SimpleBattery.CHARGE, SimpleBattery.DISCHARGE, SimpleBattery.HOLD])\n",
    "    rewards_list = []\n",
    "    history_list: List = []\n",
    "\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            history_list.append([i] + [total_rewards] + [action] + state)\n",
    "            state = next_state\n",
    "\n",
    "        # print(f\"Episode {i+1} ({env.counter}):{total_rewards}\")\n",
    "        rewards_list.append(total_rewards)\n",
    "\n",
    "        \n",
    "    average_reward = sum(rewards_list) / len(rewards_list)\n",
    "    print(f\"Average reward: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9108a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reward(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad1581",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2\n",
    "    \n",
    "- Agent discharge (sell:1) when price is higher than cost, and charge (buy:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedace6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "df_eval = evaluate_episode(agent, env_config)\n",
    "fig = plot_analysis(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv(\"result_price_vs_cost_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a00e91",
   "metadata": {},
   "source": [
    "## (2) Market Price vs Historical price Agent\n",
    "\n",
    "- SELL: when market price is higher than past 5 days average price\n",
    "- BUY: when market price is lower than past 5 days average price\n",
    "- HOLD: others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    Buy: market price < past last x days average price\n",
    "    Sell: market price > past last x days average price\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: List):\n",
    "        self.actions = actions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        market_price = state[2]\n",
    "        past_average_price = sum(state[-5:]) / len(state[-5:])\n",
    "\n",
    "        if market_price > past_average_price:\n",
    "            action = SimpleBattery.DISCHARGE\n",
    "        elif market_price < past_average_price:\n",
    "            action = SimpleBattery.CHARGE\n",
    "        else:\n",
    "            action = SimpleBattery.HOLD\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1)\n",
    "    env = SimpleBattery(env_config)\n",
    "    agent = SimpleAgent([SimpleBattery.CHARGE, SimpleBattery.DISCHARGE, SimpleBattery.HOLD])\n",
    "    rewards_list = []\n",
    "    history_list: List = []\n",
    "\n",
    "    for i in tqdm(range(EPISODE)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            history_list.append([i] + [total_rewards] + [action] + state)\n",
    "            state = next_state\n",
    "\n",
    "        # print(f\"Episode {i+1} ({env.counter}):{total_rewards}\")\n",
    "        rewards_list.append(total_rewards)\n",
    "\n",
    "    average_reward = sum(rewards_list) / len(rewards_list)\n",
    "    print(f\"Average reward: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reward(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3d559",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2\n",
    "    \n",
    "- Agent will start selling when market price is increasing (high than last 5 days average), and buy when market price is dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08616f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "df_eval = evaluate_episode(agent, env_config)\n",
    "fig = plot_analysis(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv(\"result_hist_price_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf70acb",
   "metadata": {},
   "source": [
    "## (3) SageMaker RL - DQN\n",
    "\n",
    "Next is to use DQN algorithm running on SageMaker RL. Please refer to separate notebook for more info.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
