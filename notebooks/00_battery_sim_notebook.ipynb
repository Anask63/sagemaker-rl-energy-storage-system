{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808992f",
   "metadata": {},
   "source": [
    "<div style=\"font-size:200%;font-weight:bold\">Energy Storage System</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bbc13",
   "metadata": {},
   "source": [
    "This notebook demontrates how to train an RL agent for Energy Storage System (ESS) arbitrage. The simulated energy environment is created based on the paper [Arbitrage of Energy Storage in Electricity Markets with Deep Reinforcement Learning](https://arxiv.org/abs/1904.12232), and with [this sample dataset](https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1b75f-0151-4cfc-876f-97d580c38c3d",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c9a98",
   "metadata": {},
   "source": [
    "Ensure that your Python virtual environment have installed the required python packages in `requirements.txt`. Then, execute the next cell to download the smaple data to a a local file called `data/sample-data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1819ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to download the sample data to a local file called data/sample-data.csv\n",
    "!mkdir data/\n",
    "!curl https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_202103_NSW1.csv > data/sample-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a37da5",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from energy_storage_system.baseline_agents import (\n",
    "    MovingAveragePriceAgent,\n",
    "    PriceVsCostAgent,\n",
    "    RandomAgent,\n",
    "    evaluate_episode,\n",
    "    train,\n",
    ")\n",
    "from energy_storage_system.envs import SimpleBattery\n",
    "from energy_storage_system.utils import plot_reward, plot_analysis\n",
    "\n",
    "np.random.seed(1)\n",
    "env_config = {\n",
    "    \"MAX_STEPS_PER_EPISODE\": 168,\n",
    "    \"LOCAL\": True,  # True means to use data from local src folder instead of S3.\n",
    "    \"FILEPATH\": \"data/sample-data.csv\"\n",
    "}\n",
    "env = SimpleBattery(env_config)\n",
    "episodes = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a816062-0001-46af-adbb-89310fdf9666",
   "metadata": {},
   "source": [
    "The next cell defines a helper function `train_eval()` to (train + evaluate + plot) an agent. This function will be used to evaluate three baseline agents:\n",
    "\n",
    "1. a random agent\n",
    "2. an agent that considers market price vs cost\n",
    "3. an agent that considers the moving average of market price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f833c-5521-4e2d-bda6-737def03dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(env, agent, episodes, env_config) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to train, evaluate, and plot.\"\"\"\n",
    "    # Training\n",
    "    train_results = train(env, agent, episodes)\n",
    "    plot_reward(train_results.rewards_list)  # Jupyter autoplots the returned fig\n",
    "    print(\"Average rewards across training episodes:\", train_results.mean_rewards)\n",
    "\n",
    "    # Evaluation\n",
    "    df_eval = evaluate_episode(agent, env_config)\n",
    "    plot_analysis(df_eval)  # Jupyter autoplots the returned fig\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51e750",
   "metadata": {},
   "source": [
    "# Random Agent\n",
    "\n",
    "Train an agent who behaves randomly.\n",
    "\n",
    "**Policy evaluation and observation**: the agent action is totally random, regardless of price and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_random = train_eval(env, RandomAgent(), episodes, env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29b733",
   "metadata": {},
   "source": [
    "# Market price vs cost agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than cost\n",
    "- BUY: when market price is lower than cost\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: agent discharges (sell:1) when price is higher than cost, and charges (buy:0)\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f9846-be1c-44ee-8f7c-e1cb79a8e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_price_vs_cost = train_eval(env, PriceVsCostAgent(), episodes, env_config)\n",
    "\n",
    "# Save the evaluation episode.\n",
    "df_eval_price_vs_cost.to_csv(\"result_price_vs_cost_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a00e91",
   "metadata": {},
   "source": [
    "# Market Price vs Historical price Agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than past 5 days average price\n",
    "- BUY: when market price is lower than past 5 days average price\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: Agent will start selling when market price is increasing (high than last 5 days average), and buy when market price is dropping.\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa9903-09d1-4e68-81aa-a93b656278a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_ma = train_eval(env, MovingAveragePriceAgent(), episodes, env_config)\n",
    "\n",
    "# Save the evaluation episode.\n",
    "df_eval_ma.to_csv(\"result_hist_price_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf70acb",
   "metadata": {},
   "source": [
    "# SageMaker RL - DQN\n",
    "\n",
    "Next is to use DQN algorithm running on SageMaker RL. Please refer to separate notebook for more info.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
