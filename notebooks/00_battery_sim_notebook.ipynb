{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c39d2fe",
   "metadata": {},
   "source": [
    "<div style=\"font-size:200%;font-weight:bold\">Energy Storage System</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657b846",
   "metadata": {},
   "source": [
    "This notebook demontrates how to train an RL agent for Energy Storage System (ESS) arbitrage. The simulated energy environment is created based on the paper [Arbitrage of Energy Storage in Electricity Markets with Deep Reinforcement Learning](https://arxiv.org/abs/1904.12232), and with [this sample dataset](https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b16f5",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01010f",
   "metadata": {},
   "source": [
    "Ensure that your Python virtual environment have installed the required python packages in `requirements.txt`. Then, execute the next cell to download the smaple data to a a local file called `data/sample-data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c594901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 68300  100 68300    0     0   866k      0 --:--:-- --:--:-- --:--:--  866k\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to download the sample data to a local file called data/sample-data.csv\n",
    "!mkdir data/\n",
    "!curl https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_202103_NSW1.csv > data/sample-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbe4b2",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a551d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Data size: (737, 3)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from energy_storage_system.agents import MovingAveragePriceAgent, PriceVsCostAgent, RandomAgent\n",
    "from energy_storage_system.envs import SimpleBattery\n",
    "from energy_storage_system.utils import evaluate_episode, plot_reward, plot_analysis, train\n",
    "\n",
    "np.random.seed(1)\n",
    "env_config = {\n",
    "    \"MAX_STEPS_PER_EPISODE\": 168,\n",
    "    \"LOCAL\": True,  # True means to use data from local src folder instead of S3.\n",
    "    \"FILEPATH\": \"data/sample-data.csv\"\n",
    "}\n",
    "env = SimpleBattery(env_config)\n",
    "episodes = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b41a51",
   "metadata": {},
   "source": [
    "The next cell defines a helper function `train_eval()` to (train + evaluate + plot) an agent. This function will be used to evaluate three baseline agents:\n",
    "\n",
    "1. a random agent\n",
    "2. an agent that considers market price vs cost\n",
    "3. an agent that considers the moving average of market price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e418bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(env, agent, episodes) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to train, evaluate, and plot.\"\"\"\n",
    "    # Training\n",
    "    train_results = train(env, agent, episodes)\n",
    "    plot_reward(train_results.rewards_list)  # Jupyter autoplots the returned fig\n",
    "    print(\"Average rewards across training episodes:\", train_results.mean_rewards)\n",
    "\n",
    "    # Evaluation\n",
    "    df_eval = evaluate_episode(agent, env)\n",
    "    plot_analysis(df_eval)  # Jupyter autoplots the returned fig\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca85df4",
   "metadata": {},
   "source": [
    "# Random Agent\n",
    "\n",
    "Train an agent who behaves randomly.\n",
    "\n",
    "**Policy evaluation and observation**: the agent action is totally random, regardless of price and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b001681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_random = train_eval(env, RandomAgent(), episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebf816",
   "metadata": {},
   "source": [
    "# Market price vs cost agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than cost\n",
    "- BUY: when market price is lower than cost\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: agent discharges (sell:1) when price is higher than cost, and charges (buy:0)\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_price_vs_cost = train_eval(env, PriceVsCostAgent(), episodes)\n",
    "\n",
    "# TODO: HAHA this is the data used by streamlit\n",
    "# Save the evaluation episode.\n",
    "df_eval_price_vs_cost.to_csv(\"result_price_vs_cost_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e4cc",
   "metadata": {},
   "source": [
    "# Market Price vs Historical price Agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than past 5 days average price\n",
    "- BUY: when market price is lower than past 5 days average price\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: Agent will start selling when market price is increasing (high than last 5 days average), and buy when market price is dropping.\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_ma = train_eval(env, MovingAveragePriceAgent(), episodes)\n",
    "\n",
    "# Save the evaluation episode.\n",
    "df_eval_ma.to_csv(\"result_hist_price_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14d4c7",
   "metadata": {},
   "source": [
    "# SageMaker RL - DQN\n",
    "\n",
    "Next is to use DQN algorithm running on SageMaker RL. Please refer to separate notebook for more info.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd95a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_custom_p37",
   "language": "python",
   "name": "conda_custom_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
